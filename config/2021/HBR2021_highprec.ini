[Paths]
import_folder=data/HBR/inputs/texts
database=data/HBR/database/db.h5
pretrained_bert=data/HBR/pretrained_bert
trained_berts=output/HBR_high_precision/trained_berts
processing_cache=output/HBR_high_precision/process_cache
log=output/HBR_high_precision/log
csv_outputs=output/HBR_high_precision/csv_output

[NeoConfig]
db_uri =bolt://localhost:7687
db_db = neo4j
db_pwd = nlp
protocol = bolt
http_uri = http://localhost:7474

[General]
logging_level = 10
split_hierarchy=["year"]
neo_batch_size = 100000

[Preprocessing]
max_seq_length = 80
number_params=1
char_mult=10
split_symbol=_
exclude_list=["error"]
overwrite_text_db = True

[BertTraining]
new_word_cutoff=5
mlm_probability=0.1
max_seq_length = 80
gpu_batch = 120
epochs = 10000
warmup_steps = 0
save_steps = 50000
eval_steps=500
eval_loss_limit=0.1
loss_limit = 0.09

[Processing]
cutoff_percent=90
max_degree=25
batch_size=20
prune_missing_tokens = True
maxn = 0
nr_workers = 0
cutoff_prob = 0.001
sentiment = True
pos_tagging = True
