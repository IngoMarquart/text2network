[Paths]
import_folder=E:/HBRdata/articles/
database=E:/NLPHBR/database/db.h5
pretrained_bert=E:/NLPHBR/pretrained_bert
trained_berts=E:/NLPHBR/trained_berts
processing_cache=E:/NLPHBR/process_cache
[NeoConfig]
db_uri = http://localhost:7474
db_db = neo4j
db_pwd = nlp

[General]
logging_level = 10
split_hierarchy=["year"]

[Preprocessing]
max_seq_length = 40
number_params=3
char_mult=10
split_symbol=_

[BertTraining]
mlm_probability=0.2
max_seq_length = 40
loss_limit = 0.15
gpu_batch = 50
epochs = 1000
warmup_steps = 0
save_steps = 500000
eval_steps=5000
eval_loss_limit=0.2

[Processing]
cutoff_percent=90
context_cutoff_percent=80
max_degree=25
context_max_degree=10
batch_size=15
