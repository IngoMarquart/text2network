{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "# Quick introduction to text2network package\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "To run all elements correctly, the required python packages need to be installed. That is:\n",
    "\n",
    "- Generally: PyTorch & Numpy\n",
    "- Preprocessing: tables (hdf5)\n",
    "- Analysis: networkx & python-louvain & pandas\n",
    "\n",
    "Finally, a Neo4j server, http accessible, version 4.02, should be running for processing. We currently use a custom http connector, which \n",
    "is faster than the default interface. Sadly, the connector does not work for versions above 4.02.\n",
    "We are in the process of upgrading to a standard Bolt connector.\n",
    "You can choose the version of the database in the Neo4j Desktop App.\n",
    "\n",
    "## Components\n",
    "\n",
    "This package is organized into four classes, corresponding to four steps of analysis.\n",
    "\n",
    "1. Preprocessing: Read files from folders, process text and save into database\n",
    "\n",
    "2. Training of BERT models: According to a chosen hierarchy, train a number of BERT models\n",
    "\n",
    "3. Processing / Network extraction: Use trained models to extract semantic networks and save into \n",
    "a Neo4j graph database.\n",
    "\n",
    "4. Network analysis: Condition and work with the data in the Neo4j graph database.\n",
    "\n",
    "We begin by setting up the configuration of the project. This configuration file can be passed along to all classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Configuration\n",
    "\n",
    "We use the standard python configuration parser to read an ini file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configuration_path='D:/NLP/COCA/cocaBERT/config/config.ini'\n",
    "\n",
    "# Load Configuration file\n",
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read(configuration_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside the configuration file are a number of fields. \n",
    "It is essential to set the correct paths.\n",
    "\n",
    "<code>import_folder</code> holds the txt data\n",
    "\n",
    "<code>pretrained_bert</code> holds the pre-trained BERT Pytorch model, that is used for all divisions of the corpus.\n",
    "\n",
    "<code>trained_berts</code> will store the fine-tuned BERT models.\n",
    "\n",
    "<code>processing_cache</code> simply keeps track of which subcorpus has already been processed into the Neo4j Graph-\n",
    "\n",
    "<code>database</code> holds the processed text in a hdf5 database.\n",
    "\n",
    "<code>log</code> is the folder for the log.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "\n",
    "Once text files are comfortably situated in a folder, the text can be pre-processed.\n",
    "Sentences that are too long are split, tags and other nuisance characters are deleted and so forth.\n",
    "\n",
    "Most importantly, each sentence is saved in a database, together with its metadata.\n",
    "This always includes the following:\n",
    "\n",
    "<code>Year</code>: A time variable integer. Typically, YYYY, but YYYYMMDD could be used.\n",
    "\n",
    "<code>Source</code>: Name of the txt file\n",
    "\n",
    "<code>p1 through p4</code>: Up to four parameters coming from the file name\n",
    "\n",
    "<code>run_index</code> An index across all sentences in all text files.\n",
    "\n",
    "<code>seq_id</code> An index across sentences within a given text file.\n",
    "\n",
    "<code>text</code> The sentence, capped at a maximum length of characters.\n",
    "\n",
    "Since each sentence is then saved as a row in the database, we can determine at a later\n",
    "stage how we seek to query and split the corpus into subcorpora (e.g. by year and parameter 1).\n",
    "\n",
    "So initially, we need to use the configuration file to define the properties of the text we are going to use. In particular,\n",
    "we need to define what the file names mean.\n",
    "Two options for the file structure are possible:\n",
    "\n",
    "First, the import folder could include sub-folders of years.\n",
    "\n",
    "    import_folder/\n",
    "        import_folder/year1/\n",
    "        ------p1_p2_p3_p4.txt\n",
    "        ------p1_p2_p3_p4.txt\n",
    "        (...)\n",
    "        import_folder/year2/\n",
    "        ------p1_p2_p3_p4.txt\n",
    "        ------p1_p2_p3_p4.txt\n",
    "        (...)\n",
    "     \n",
    "Alternatively, all txt files can also reside in a single folder.\n",
    "\n",
    "    import_folder/\n",
    "        ------year1_p1_p2_p3_p4.txt\n",
    "        ------year1_p1_p2_p3_p4.txt\n",
    "        ------year2_p1_p2_p3_p4.txt\n",
    "        ------year3_p1_p2_p3_p4.txt\n",
    "        (...)\n",
    "        \n",
    "Accordingly, we set the following parameters in the configuration file: <code>split_symbol</code>\n",
    "is the symbol that splits between parameters (here \"\\_\"). <code>number_params</code> denotes the number\n",
    "of parameters (here 4). If we had only two parameters, our text files might be\n",
    "of the form <code>p1_p2.txt</code> and we would set that value to 2.\n",
    "Finally, <code>max_seq_length</code> denotes the maximum length of a sentence.\n",
    "<code>char_mult</code> is a multiplier that determines how many letters the average word can have.\n",
    "The total sequence length in letters (symbols) is given by <code>max_seq_length*char_mult</code>.\n",
    "Having a fixed-length format here is helpful for performance. Sequences can, of course, be shorter. Later components \n",
    "will also re-split sentences if smaller batch sizes are desired. Setting the sequence size very high\n",
    "ensures that no sentence will be unduly split, however this will increase file size.\n",
    "\n",
    "We begin by instancing the preprocessing class.\n",
    "At this stage, we will also set up logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from src.classes.nw_preprocessor import nw_preprocessor\n",
    "\n",
    "# Set up preprocessor\n",
    "preprocessor = nw_preprocessor(config)\n",
    "# Set up logging\n",
    "preprocessor.setup_logger()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that is is sufficient to pass the <code>config</code>, however the class also\n",
    "takes optional parameters, if we want to overwrite the configuration file.\n",
    "This is the standard behavior for all modules. So for example one could instead do:\n",
    "\n",
    "    preprocessor = nw_preprocessor(config, max_seq_length=50)\n",
    "\n",
    "\n",
    "Next, we can process the text files and create the database.\n",
    "If our text files are split among multiple sub-folders, with years as folder names,\n",
    "we call the <code>preprocess_folders</code> method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "preprocessor.preprocess_folders(overwrite=True,excludelist=['checked', 'Error'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here, <code>overwrite</code> indicates that we wish to overwrite any existing database.\n",
    "<code>excludelist</code> is a list of strings corresponding to any of the parameters\n",
    "in the file name. Filenames including elements from this list are not processed.\n",
    "\n",
    "If, instead, all files are in a single folder, we run\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "preprocessor.preprocess_files(overwrite=True,excludelist=['checked', 'Error'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that both functions also take a <code>folder</code> variable, if we want to not use the folder of the configuration file.\n",
    "In this way, the pre-processing can also be done across many sources. Note, however, that the \n",
    "file name of the txt file is essential and needs to follow the same convention:\n",
    "Either folders with year names, or files starting with years, and then up to four parameters.\n",
    "\n",
    "The module will try to take care of encodings and other matters. If the file can not be read, an error will be \n",
    "returned.\n",
    "\n",
    "Once done, a <code>db.h5</code> file will be created in the <code>database</code> folder, which includes all\n",
    "individual sentences and their meta-data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training BERT\n",
    "\n",
    "\n",
    "## Understanding split hierarchy\n",
    "\n",
    "We will train one BERT model for each logical division of the corpus. This sub-division will be carried along all subsequent steps. So, processing a certain subdivision requires that a corresponding BERT model has been trained. Different divisions can be trained and saved, as they will be saved in distinct folders.\n",
    "\n",
    "Subdivisions are specified via the <code>split_hierarchy</code> option in the configuration file.\n",
    "\n",
    "It is a list of parameters by which to split the corpus and train the models. All parameters are always saved as meta-data, but we might want to aggregate across them when training BERT.\n",
    "\n",
    "The simplest division is by year:\n",
    "\n",
    "    split_hierarchy=[\"year\"]\n",
    "\n",
    "This will train one BERT per year.\n",
    "However, we might also train one BERT per combination of year, p1 and p2, e.g.\n",
    "\n",
    "    split_hierarchy=[\"year\",\"p1\",p2\"]\n",
    "\n",
    "\n",
    "By setting this parameter, the trainer module can ascertain how many BERTs are required, and which sentences it should train on.\n",
    "\n",
    "## Training process\n",
    "\n",
    "We do not wish to use word-pieces. The pre-trained BERT has word-pieces disabled. For that reason, the vocabulary needs to be amended. It is desirable, although not strictly necessary, to use the same vocabulary across all models. To keep this reasonable, set <code>new_word_cutoff</code> for large corpora. Only words that occur more often will be included in the vocabulary.\n",
    "\n",
    "The training process creates first one shared vocabulary, resizes the BERT models and then trains them individually.\n",
    "\n",
    "Each model is trained until either <code>eval_loss_limit</code> or <code>loss_limit</code> is reached, where the first denotes the loss across test sequences, whereas the second in the current batch during training. The configuration file also includes the usual model parameters, that should be set according to GPU size and corpus size.\n",
    "\n",
    "To train all BERTs, we initialize the trainer and run the training.\n",
    "Again, attributes may be given via the config file or as individual parameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from src.classes.bert_trainer import bert_trainer\n",
    "\n",
    "trainer=bert_trainer(config)\n",
    "trainer.train_berts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network processing\n",
    "\n",
    "Having trained BERTs, we need to extract semantic networks. This involves running inference across the subdivisions of the corpus and saving network ties in the Neo4j database.\n",
    "\n",
    "All interfacing with Neo4j is done via the network class, which we initialize first.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from src.classes.neo4jnw import neo4j_network\n",
    "neograph = neo4j_network(config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network is, of course, entirely empty at this stage. To fill it, we also create a processer that takes the network interface as input.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from src.classes.nw_processor import nw_processor\n",
    "processor = nw_processor(config, neograph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since all options are already specified in the configuration file, we can directly process our semantic networks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "processor.run_all_queries(delete_incomplete=True, delete_all=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where we can specify whether we would like to clean the graph database first - in order not to duplicate ties - or not.\n",
    "\n",
    "Note that the processor remembers whether a BERT model has already been processed to completion. By specifying <code>delete_incomplete</code>, the processor will first clean the graph database of subdivisions that were not completed.\n",
    "This is useful if the processing gets interrupted.\n",
    "\n",
    "Conversely, <code>delete_all</code> cleans the graph entirely for a fresh start.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}